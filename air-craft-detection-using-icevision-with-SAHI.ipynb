{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Air Craft Detection using IceVision","metadata":{"id":"gDWmMZgCJTvy"}},{"cell_type":"markdown","source":"## Installing IceVision and IceData\n","metadata":{"id":"kEoDOD1spe0m"}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation\n\n# Choose your installation target: cuda11 or cuda10 or cpu\n!bash icevision_install.sh cuda11 master\n","metadata":{"id":"_Pp3o0wMpe0t","outputId":"6cffe7a7-b9db-4562-c222-3dd77d74c2c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import IPython\nIPython.Application.instance().kernel.do_shutdown(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"CgCtcut8pe0u"}},{"cell_type":"code","source":"# !pip install --upgrade -q wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sahi -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# wandb_api = user_secrets.get_secret(\"wandb_api\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import wandb\n# from wandb.keras import WandbCallback\n# wandb.login(key=wandb_api)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from icevision.all import *\nimport os\nimport ast\nimport random\nimport pandas as pd\nfrom fastai.callback.wandb import *\nfrom fastai.callback.tracker import SaveModelCallback\nfrom icevision.imports import *\nfrom icevision.utils import *\nfrom icevision.data import *\nfrom icevision.metrics.metric import *\nfrom icevision.models.inference_sahi import IceSahiModel","metadata":{"id":"24H5UQe3pe0u","outputId":"b381ee79-4bec-47b0-c658-96bdabba9578","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/airbus-aircrafts-sample-dataset ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"id":"iRPp9AGvh4MR","outputId":"7c3037d9-f0c8-492a-eb6a-ade8391d06e4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"id":"iY3F9MF1h6kl","outputId":"e1a96f2d-ba30-4963-a7a1-44ac1438ea03","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visual Inspection","metadata":{"id":"uyCynCl6pzKi"}},{"cell_type":"code","source":"DATA_DIR = Path('./airbus-aircrafts-sample-dataset')\nimg_list = list(DATA_DIR.glob('images/*.jpg'))\npickone = random.choice(img_list)\nimg = PIL.Image.open(pickone)\ndisplay(img)","metadata":{"id":"jJdBLws1iLoI","outputId":"91ef8c4c-fb57-4384-a5bb-ea3e05da3c7a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"only_files = [DATA_DIR / f.name for f in img_list if os.path.isfile(f) and f.name[-4:] == \".jpg\"]\nprint(\"Found {} images files in {}\".format(len(only_files), DATA_DIR))\n\nIMAGE_HEIGHT, IMAGE_WIDTH = img.size\nnum_channels = len(img.mode)\nprint(\"Image size: {}\".format((IMAGE_HEIGHT, IMAGE_WIDTH)))\nprint(\"Num channels: {}\".format(num_channels))","metadata":{"id":"EnRBsi05jAM8","outputId":"422ea100-e587-4b7a-fe72-ef69f79446b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read CSV file","metadata":{"id":"tKE-717np7bm"}},{"cell_type":"code","source":"df = pd.read_csv(DATA_DIR / 'annotations.csv')","metadata":{"id":"3SkGRvfZjQU6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `geometry` is encoded as string\n","metadata":{"id":"T-PyUIM9qCeq"}},{"cell_type":"code","source":"geometry = df.loc[0]['geometry']\ngeometry","metadata":{"id":"rQgbeUT2uvFi","outputId":"b7105833-6ad0-41c0-e074-aa734c81622f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"geometry2 = ast.literal_eval(geometry.rstrip('\\r\\n'))\ngeometry2","metadata":{"id":"hTedlhHeu4t4","outputId":"7ae56f0e-0b35-4739-d169-489d7d34a8fc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting `geometry` column to a proper format","metadata":{"id":"at3z92DLqPED"}},{"cell_type":"code","source":"# convert a string record into a valid python object\ndef f(x): \n    return ast.literal_eval(x.rstrip('\\r\\n'))\n\ndf = pd.read_csv(DATA_DIR / \"annotations.csv\", \n                converters={'geometry': f})\ndf.head(10)","metadata":{"id":"Mek26eOYuqI6","outputId":"89be134e-4e08-4b39-be26-2a6a1eafbcc5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calcutation the Bounding Box `(xmin, ymin, xmax, ymax)`","metadata":{"id":"REvr_pIkqjfh"}},{"cell_type":"code","source":"geometry = df.loc[0]['geometry']\ngeometry","metadata":{"id":"0GNgRXzAjaAm","outputId":"75c7a5ea-a343-427b-d2ea-8b9e23ec1ab7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr = np.array(geometry).T","metadata":{"id":"to_Av39Ujx9h","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xmin = np.min(arr[0])\nymin = np.min(arr[1])\nxmax = np.max(arr[0])\nymax = np.max(arr[1])\n(xmin, ymin, xmax, ymax)","metadata":{"id":"6BS2sSXPkNIt","outputId":"943122ee-e975-443f-e196-1dc53feb761b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Updating the dataframe ","metadata":{"id":"9-j4cMBcrI0n"}},{"cell_type":"code","source":"def getBounds(geometry):\n    try: \n        arr = np.array(geometry).T\n        xmin = np.min(arr[0])\n        ymin = np.min(arr[1])\n        xmax = np.max(arr[0])\n        ymax = np.max(arr[1])\n        return (xmin, ymin, xmax, ymax)\n    except:\n        return np.nan\n\ndef getWidth(bounds):\n    try: \n        (xmin, ymin, xmax, ymax) = bounds\n        return np.abs(xmax - xmin)\n    except:\n        return np.nan\n\ndef getHeight(bounds):\n    try: \n        (xmin, ymin, xmax, ymax) = bounds\n        return np.abs(ymax - ymin)\n    except:\n        return np.nan\n\ndf.rename(columns={'class':'label'}, inplace=True) # to avoid semantic conflicts\n\n# Create bounds, width and height\ndf.loc[:,'bounds'] = df.loc[:,'geometry'].apply(getBounds)\ndf.loc[:,'width'] = df.loc[:,'bounds'].apply(getWidth)\ndf.loc[:,'height'] = df.loc[:,'bounds'].apply(getHeight)\ndf.head(10)","metadata":{"id":"DKUD8O7IxHZ1","outputId":"35b13cb6-c997-418e-f331-b0a2727862f9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the Parser","metadata":{"id":"NglxnDFrJTv9"}},{"cell_type":"markdown","source":"The first step is to create a template record for our specific type of dataset, in this case we're doing standard object detection:","metadata":{"id":"8MjXpkgcJTv9"}},{"cell_type":"code","source":"template_record = ObjectDetectionRecord()","metadata":{"id":"YbOv-5oSJTv-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now use the method `generate_template` that will print out all the necessary steps we have to implement.","metadata":{"id":"ux0nzaorJTv-"}},{"cell_type":"code","source":"Parser.generate_template(template_record)","metadata":{"id":"DHorsv0gJTv-","outputId":"fefd7931-bdd0-4942-e3cc-d2180ed92f0d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can copy the template and use it as our starting point. Let's go over each of the methods we have to define:\n\n- `__init__`: What happens here is completely up to you, normally we have to pass some reference to our data, `data_dir` in our case.\n\n- `__iter__`: This tells our parser how to iterate over our data, each item returned here will be passed to `parse_fields` as `o`. In our case we call `df.itertuples` to iterate over all `df` rows.\n\n- `__len__`: How many items will be iterating over.\n\n- `imageid`: Should return a `Hashable` (`int`, `str`, etc). In our case we want all the dataset items that have the same `filename` to be unified in the same record.\n\n- `parse_fields`: Here is where the attributes of the record are collected, the template will suggest what methods we need to call on the record and what parameters it expects. The parameter `o` it receives is the item returned by `__iter__`.","metadata":{"id":"Ow0_MEaEJTv_"}},{"cell_type":"markdown","source":"!!! danger \"Important\"  \n    Be sure to pass the correct type on all record methods!","metadata":{"id":"Y8256dZTJTv_"}},{"cell_type":"code","source":"class AirbusParser(Parser):\n    # convert a string record into a valid python object\n    def convert_fn(x): \n        return ast.literal_eval(x.rstrip('\\r\\n'))\n\n    def __init__(self, template_record, data_dir, df):\n        super().__init__(template_record=template_record)\n        \n        self.data_dir = data_dir\n        # self.df = pd.read_csv(data_dir / \"annotations.csv\", converters={'geometry': self.convert_fn})\n        self.df = df\n\n        self.class_map = ClassMap(list(self.df['label'].unique()))\n        \n\n    def __iter__(self) -> Any:\n        for o in self.df.itertuples():\n            yield o\n        \n    def __len__(self) -> int:\n        return len(self.df)\n        \n    def record_id(self, o) -> Hashable:\n        return o.image_id\n        \n    def parse_fields(self, o, record, is_new):\n        if is_new:\n            \n            filepath = self.data_dir / 'images' / o.image_id\n            record.set_filepath(filepath)\n            \n            if filepath.exists():\n              image_size = get_img_size(filepath)\n              record.set_img_size(image_size)\n            \n            record.detection.set_class_map(self.class_map)\n        \n\n        (xmin, ymin, xmax, ymax) = o.bounds\n        record.detection.add_bboxes([BBox.from_xyxy(xmin, ymin, xmax, ymax)])\n        record.detection.add_labels([o.label])","metadata":{"id":"BXQdQLosJTwA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's randomly split the data and parser with `Parser.parse`:","metadata":{"id":"arHi8iZwJTwA"}},{"cell_type":"code","source":"data_dir = Path('./airbus-aircrafts-sample-dataset')\ndata_dir","metadata":{"id":"tkqJKPyCpHte","outputId":"1f64eb91-c72d-42e4-8dbb-53b147763ebc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir.ls()","metadata":{"id":"t5bLIX3UqpzK","outputId":"c762a93b-fb3c-4cd8-f14b-61ed97cd35d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser = AirbusParser(template_record, data_dir, df)","metadata":{"id":"YetvB63uJTwB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_records, valid_records = parser.parse()","metadata":{"id":"90cKzJFmJTwB","outputId":"380ddb89-4c35-4df6-b3ae-dcd8d64cc474","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at one record:","metadata":{"id":"qROqFYjJJTwB"}},{"cell_type":"code","source":"show_record(train_records[0], display_label=False, figsize=(14, 10))","metadata":{"id":"dwEk3eOfJTwB","outputId":"28117194-2566-4d21-bb3b-e8af27c7684d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_records[0]","metadata":{"id":"8L06uXSOJTwC","outputId":"5cff69d0-a4ea-4e02-8ebe-d910f21ee607","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforms\n# size is set to 384 because EfficientDet requires its inputs to be divisible by 128\n# Try image_size = 512 or image_size = 640 or image_size = 768 (incrementing by 128px)\nimage_size = 640\npresize = image_size + 128\ntrain_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=image_size, presize=presize), tfms.A.Normalize()])\nvalid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(image_size), tfms.A.Normalize()])","metadata":{"id":"V-KNky2UNiY0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Datasets\ntrain_ds = Dataset(train_records, train_tfms)\nvalid_ds = Dataset(valid_records, valid_tfms)","metadata":{"id":"AFXklID_NetU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show an element of the train_ds with augmentation transformations applied\nsamples = [train_ds[0] for _ in range(3)]\nshow_samples(samples, ncols=3)","metadata":{"id":"KYQb1mK1UzoY","outputId":"561114df-7051-4bdb-c132-5898af6c3ef0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models","metadata":{"id":"A2DVFSmrJTwC"}},{"cell_type":"markdown","source":"We've selected a few of the many options below. You can easily pick which libraries, models, and backbones you like to use.","metadata":{"id":"rvv7i988JTwC"}},{"cell_type":"code","source":"# Just change the value of selection to try another model\n\nselection = 13\n\nextra_args = {}\n\nif selection == 0:\n  model_type = models.mmdet.vfnet\n  backbone = model_type.backbones.resnet50_fpn_mstrain_2x\n\nif selection == 1:\n  model_type = models.mmdet.retinanet\n  backbone = model_type.backbones.resnet50_fpn_1x\n  # extra_args['cfg_options'] = { \n  #   'model.bbox_head.loss_bbox.loss_weight': 2,\n  #   'model.bbox_head.loss_cls.loss_weight': 0.8,\n  #    }\n\nif selection == 2:\n  model_type = models.mmdet.faster_rcnn\n  backbone = model_type.backbones.resnet101_fpn_2x\n  # extra_args['cfg_options'] = { \n  #   'model.roi_head.bbox_head.loss_bbox.loss_weight': 2,\n  #   'model.roi_head.bbox_head.loss_cls.loss_weight': 0.8,\n  #    }\n\nif selection == 3:\n  model_type = models.mmdet.ssd\n  backbone = model_type.backbones.ssd300\n\nif selection == 4:\n  model_type = models.mmdet.yolox\n  backbone = model_type.backbones.yolox_s_8x8\n\nif selection == 5:\n  model_type = models.mmdet.yolof\n  backbone = model_type.backbones.yolof_r50_c5_8x8_1x_coco\n\nif selection == 6:\n  model_type = models.mmdet.detr\n  backbone = model_type.backbones.r50_8x2_150e_coco\n\nif selection == 7:\n  model_type = models.mmdet.deformable_detr\n  backbone = model_type.backbones.twostage_refine_r50_16x2_50e_coco\n\nif selection == 8:\n  model_type = models.mmdet.fsaf\n  backbone = model_type.backbones.x101_64x4d_fpn_1x_coco\n\nif selection == 9:\n  model_type = models.mmdet.sabl\n  backbone = model_type.backbones.r101_fpn_gn_2x_ms_640_800_coco\n\nif selection == 10:\n  model_type = models.mmdet.centripetalnet\n  backbone = model_type.backbones.hourglass104_mstest_16x6_210e_coco\n\nelif selection == 11:\n  # The Retinanet model is also implemented in the torchvision library\n  model_type = models.torchvision.retinanet\n  backbone = model_type.backbones.resnet50_fpn\n\nelif selection == 12:\n  model_type = models.ross.efficientdet\n  backbone = model_type.backbones.tf_lite0\n  # The efficientdet model requires an img_size parameter\n  extra_args['img_size'] = image_size\n\nelif selection == 13:\n  model_type = models.ultralytics.yolov5\n  backbone = model_type.backbones.large\n  # The yolov5 model requires an img_size parameter\n  extra_args['img_size'] = image_size\n\nmodel_type, backbone, extra_args","metadata":{"id":"yPwdFgbtMPlw","outputId":"59746597-38cd-45b5-9c0b-2ac1c16eb449","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model\nmodel = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), **extra_args) ","metadata":{"id":"2SeCwdaMNBbO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Loaders\ntrain_dl = model_type.train_dl(train_ds, batch_size=8, num_workers=4, shuffle=True)\nvalid_dl = model_type.valid_dl(valid_ds, batch_size=8, num_workers=4, shuffle=False)","metadata":{"id":"QBNoSItvNJej","outputId":"52aa88d3-ba43-4252-a618-b4f3b2a7d1ff","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"T5j26PrNN7ZW"}},{"cell_type":"code","source":"class COCOMetricType(Enum):\n    \"\"\"Available options for `COCOMetric`.\"\"\"\n\n    bbox = \"bbox\"\n    mask = \"segm\"\n    keypoint = \"keypoints\"\n\n\nclass COCOMetric(Metric):\n    \"\"\"Wrapper around [cocoapi evaluator](https://github.com/cocodataset/cocoapi)\n    Calculates average precision.\n    # Arguments\n        metric_type: Dependent on the task you're solving.\n        print_summary: If `True`, prints a table with statistics.\n        show_pbar: If `True` shows pbar when preparing the data for evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric_type: COCOMetricType = COCOMetricType.bbox,\n        iou_thresholds: Optional[Sequence[float]] = None,\n        print_summary: bool = False,\n        show_pbar: bool = False,\n    ):\n        self.metric_type = metric_type\n        self.iou_thresholds = iou_thresholds\n        self.print_summary = print_summary\n        self.show_pbar = show_pbar\n        self._records, self._preds = [], []\n\n    def _reset(self):\n        self._records.clear()\n        self._preds.clear()\n\n    def accumulate(self, preds):\n        for pred in preds:\n            self._records.append(pred.ground_truth)\n            self._preds.append(pred.pred)\n\n    def finalize(self) -> Dict[str, float]:\n        with CaptureStdout():\n            coco_eval = create_coco_eval(\n                records=self._records,\n                preds=self._preds,\n                metric_type=self.metric_type.value,\n                iou_thresholds=self.iou_thresholds,\n                show_pbar=self.show_pbar,\n            )\n            coco_eval.evaluate()\n            coco_eval.accumulate()\n\n        with CaptureStdout(propagate_stdout=self.print_summary):\n            coco_eval.summarize()\n\n        stats = coco_eval.stats\n        logs = {\n            \"AP (IoU=0.50:0.95) area=all\": stats[0],\n            \"AP (IoU=0.50) area=all\": stats[1],\n            \"AP (IoU=0.75) area=all\": stats[2],\n            \"AP (IoU=0.50:0.95) area=small\": stats[3],\n            \"AP (IoU=0.50:0.95) area=medium\": stats[4],\n            \"AP (IoU=0.50:0.95) area=large\": stats[5],\n            \"AR (IoU=0.50:0.95) area=all maxDets=1\": stats[6],\n            \"AR (IoU=0.50:0.95) area=all maxDets=10\": stats[7],\n            \"AR (IoU=0.50:0.95) area=all maxDets=100\": stats[8],\n            \"AR (IoU=0.50:0.95) area=small maxDets=100\": stats[9],\n            \"AR (IoU=0.50:0.95) area=medium maxDets=100\": stats[10],\n            \"AR (IoU=0.50:0.95) area=large maxDets=100\": stats[11],\n        }\n\n        self._reset()\n        return logs","metadata":{"id":"0Sy53qJ7zkgW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]","metadata":{"id":"Jah_pPMzN5hH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training using fastai","metadata":{"id":"0j15EQiINv8l"}},{"cell_type":"code","source":"# wandb.init(project=\"aircraft-detection\", name=\"yolov5-large-384\", reinit=True)","metadata":{"id":"7P4X3I4hhuyN","outputId":"2473800e-5d2d-40cc-f449-58e480dee878","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)\n#cbs=[WandbCallback(), SaveModelCallback()]","metadata":{"id":"Oabig2DSNvY8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"id":"XQemxQSSOC1E","outputId":"1223bc39-eb0b-42ff-94c5-c5b971d52ca9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fine_tune(40, 0.009120108559727669, freeze_epochs=1)","metadata":{"id":"ylh7GzGzOGvz","outputId":"ef6e2b08-b6bd-4607-a5b9-7aa8d4c29277","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training using Pytorch Lightning","metadata":{"id":"gaKvJjvYOLWe"}},{"cell_type":"code","source":"# class LightModel(model_type.lightning.ModelAdapter):\n#     def configure_optimizers(self):\n#         return Adam(self.parameters(), lr=1e-4)\n    \n# light_model = LightModel(model, metrics=metrics)","metadata":{"id":"bU5bn7RHONqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer = pl.Trainer(max_epochs=20, gpus=1)\n# trainer.fit(light_model, train_dl, valid_dl)","metadata":{"id":"1_i4qyzKOSVg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Showing the results","metadata":{"id":"dqNPXLkSOaDl"}},{"cell_type":"code","source":"model_type.show_results(model, valid_ds, detection_threshold=.5)","metadata":{"id":"2bdEEyveOfAQ","outputId":"9c824640-b040-4e97-e39f-cacfd8344e9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Batch Inference (Prediction)","metadata":{"id":"lAA8V70JOlPf"}},{"cell_type":"code","source":"infer_dl = model_type.infer_dl(valid_ds, batch_size=4, shuffle=False)\npreds = model_type.predict_from_dl(model, infer_dl, keep_images=True)","metadata":{"id":"gbR0livtOp1x","outputId":"a65d6388-9d67-4c51-f2c0-4d5a8e5b98b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_preds(preds=preds[:4])","metadata":{"id":"5bxbypBuOs-X","outputId":"e1767c8e-c873-49d4-903e-58114c833084","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SAHI","metadata":{}},{"cell_type":"code","source":"sahimodel = IceSahiModel(model_type=model_type, model=model, class_map=parser.class_map, tfms=valid_tfms, confidence_threshold=0.4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_list = list(DATA_DIR.glob('images/*.jpg'))\npickone = random.choice(img_list)\nimg = PIL.Image.open(pickone)\ndisplay(img)","metadata":{"execution":{"iopub.status.busy":"2022-09-28T22:53:30.493343Z","iopub.execute_input":"2022-09-28T22:53:30.494555Z","iopub.status.idle":"2022-09-28T22:53:30.587275Z","shell.execute_reply.started":"2022-09-28T22:53:30.494423Z","shell.execute_reply":"2022-09-28T22:53:30.585806Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_38/1731351399.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/*.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpickone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DATA_DIR' is not defined"],"ename":"NameError","evalue":"name 'DATA_DIR' is not defined","output_type":"error"}]},{"cell_type":"code","source":"pred = sahimodel.get_sliced_prediction(\n                img,\n                keep_sahi_format=False,\n                return_img=True,\n                slice_height = 128,\n                slice_width = 128,\n                overlap_height_ratio = 0.2,\n                overlap_width_ratio = 0.2,\n            )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred['img']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from icevision.models.checkpoint import *\n# save_icevision_checkpoint(model,\n#                         model_name='ultralytics.yolov5', \n#                         backbone_name='medium_p6',\n#                         img_size=image_size,\n#                         classes=parser.class_map.get_classes(),\n#                         filename='./models/model_checkpoint.pth',\n#                         meta={'icevision_version': '0.12.0'})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}